# bot.py
from core.report import create_pdf_report
from core.faiss_db import FAISSDatabase
from core.plagiarism import check_plagiarism_online, calculate_similarity
from core.rewriter import rewrite_text
from core.parser import extract_text
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ContextTypes
import os
from dotenv import load_dotenv
import tempfile
import shutil
import nltk
from nltk.tokenize import sent_tokenize

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
load_dotenv()

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª–∏

# –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
nltk.download('punkt', quiet=True)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_ENGINE_ID = os.getenv("GOOGLE_ENGINE_ID")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞–∑—É FAISS
db = FAISSDatabase()


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç-–∞–Ω—Ç–∏–ø–ª–∞–≥–∏–∞—Ç.\n\n"
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "üìÑ /plagiarism ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –ø–ª–∞–≥–∏–∞—Ç\n"
        "‚úçÔ∏è /rewrite ‚Äî –ø–æ–≤—ã—Å–∏—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å\n\n"
        "–û—Ç–ø—Ä–∞–≤—å —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ñ–∞–π–ª (.txt, .docx, .pdf, .jpg –∏ –¥—Ä.)"
    )


async def plagiarism(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "üîç –û—Ç–ø—Ä–∞–≤—å —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ñ–∞–π–ª –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –ø–ª–∞–≥–∏–∞—Ç.\n"
        "–Ø –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏ –≤—ã–¥–∞–º –æ—Ç—á—ë—Ç —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏."
    )
    context.user_data['mode'] = 'plagiarism'


async def rewrite(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "üîÑ –û—Ç–ø—Ä–∞–≤—å —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ñ–∞–π–ª –¥–ª—è –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è."
    )
    context.user_data['mode'] = 'rewrite'


async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    mode = context.user_data.get('mode', 'rewrite')

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞
    if update.message.document or update.message.photo:
        if update.message.photo:
            file = await update.message.photo[-1].get_file()
        else:
            file = await update.message.document.get_file()

        file_path = await file.download_to_drive()
        text = extract_text(file_path)

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        try:
            if os.path.isdir(file_path):
                shutil.rmtree(file_path)
            else:
                os.remove(file_path)
        except:
            pass
    elif update.message.text:
        text = update.message.text
    else:
        await update.message.reply_text("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ñ–∞–π–ª.")
        return

    if len(text.strip()) < 20:
        await update.message.reply_text("–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π.")
        return

    sentences = sent_tokenize(text)[:8]

    if mode == 'plagiarism':
        await update.message.reply_text("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–∫—Å—Ç... (10‚Äì25 —Å–µ–∫)")

        total_web_sim = 0
        web_matches_all = []
        faiss_matches_all = []
        high_risk = 0

        for sent in sentences:
            if len(sent) < 15:
                continue

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ Google
            web_candidates = check_plagiarism_online(
                sent, GOOGLE_API_KEY, GOOGLE_ENGINE_ID)
            web_sim, best_link = calculate_similarity(sent, web_candidates)
            total_web_sim += web_sim

            if web_sim > 0.4:
                web_matches_all.append({
                    "text": sent,
                    "link": best_link or "#",
                    "similarity": web_sim
                })

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑–µ
            local_results = db.search(sent, k=2)
            for res in local_results:
                if res['similarity'] > 0.6:
                    faiss_matches_all.append(res)

            if web_sim > 0.7:
                high_risk += 1

        # –û—Ü–µ–Ω–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        avg_web_sim = total_web_sim / len(sentences) if sentences else 0
        plagiarism_percent = avg_web_sim * 100
        uniqueness = 100 - plagiarism_percent

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É, –µ—Å–ª–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π
        saved_name = None
        if uniqueness > 70 and len(text) > 300:
            topic = text[:50].replace(' ', '_').replace(
                '.', '').replace(',', '') + f"_{int(uniqueness)}p.txt"
            db.add_text(text, title=topic)
            saved_name = topic

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF
        try:
            create_pdf_report(
                "report.pdf",
                original=text,
                rewritten=rewrite_text(text),
                plagiarism_score=plagiarism_percent,
                web_matches=web_matches_all,
                faiss_matches=faiss_matches_all
            )
            with open("report.pdf", "rb") as f:
                await update.message.reply_document(f, filename="–û—Ç—á—ë—Ç_–∞–Ω—Ç–∏–ø–ª–∞–≥–∏–∞—Ç.pdf")
        except Exception as e:
            await update.message.reply_text(f"üìÑ PDF –Ω–µ —Å–æ–∑–¥–∞–Ω: {e}")

        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç
        result = f"üìä *–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å*: {uniqueness:.0f}%\n‚ö†Ô∏è *–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤*: {high_risk}\n\n"

        if web_matches_all:
            result += "*–ù–∞–π–¥–µ–Ω–æ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ:*\n"
            for i, m in enumerate(web_matches_all[:5], 1):
                short = m['text'][:50] + "..."
                result += f"{i}. `{short}` ‚Üí [—Å—Å—ã–ª–∫–∞]({m['link']})\n"
            result += "\n"

        if faiss_matches_all:
            result += "*–ù–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ:*\n"
            for m in faiss_matches_all[:3]:
                result += f"‚Ä¢ `{m['text'][:50]}...` ‚Üí –∏–∑ *{m['title']}*\n"
            result += "\n"

        if saved_name:
            result += f"‚úÖ –†–∞–±–æ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ `{saved_name}`\n"

        await update.message.reply_text(result, parse_mode='Markdown')
        context.user_data['mode'] = None

    elif mode == 'rewrite':
        await update.message.reply_text("üîÑ –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É—é —á–µ—Ä–µ–∑ Llama 3...")
        rewritten = rewrite_text(text)
        await update.message.reply_text(f"‚ú® –£–Ω–∏–∫–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è:\n\n{rewritten}")
        context.user_data['mode'] = None

# === –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ ===
if __name__ == '__main__':
    app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()

    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("plagiarism", plagiarism))
    app.add_handler(CommandHandler("rewrite", rewrite))
    app.add_handler(MessageHandler(
        filters.TEXT | filters.Document.ALL | filters.PHOTO, handle_message))

    print("‚úÖ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π...")
    app.run_polling()
